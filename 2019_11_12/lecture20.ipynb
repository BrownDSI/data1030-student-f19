{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mud card\n",
    "- **Can you talk about the variance/bias relationship one more time?**\n",
    "   - overfitting / high variance model / low bias model \n",
    "      - the model is too complex\n",
    "      - it performs very well on the training data (aka overfits the training data) \n",
    "      - but it performs poorly on new data points\n",
    "   - underfitting / low variance model / high bias model\n",
    "      - the model is too simple\n",
    "      - it performs poorly on the training data (aka underfits)\n",
    "      - and it also performs poorly on new data points\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Can we go over how to properly split your data again?**\n",
    "   - today and tomorrow :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross Validation with iid data\n",
    "By the end of this lecture, you will be able to\n",
    "- apply simple CV and k-fold CV to datasets\n",
    "- use GridSearchCV with pipelines\n",
    "- apply stratified splits to imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The goals of cross validation\n",
    "- we want to find the best hyper-parameters of our ML algorithms\n",
    "   - fit model to training data (`.fit(X_train,y_train)`)\n",
    "   - evaluate model on CV set (`.predict(X_CV,y_CV)`)\n",
    "   - we find hyper-parameter values that optimize the CV score\n",
    "- we want to know how the model will perform on previously unseen data\n",
    "   - apply our final model on the test set (`.predict(X_test,y_test)`)\n",
    "   \n",
    "### We need to split the data into three parts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How should we split the data into train/CV/test?\n",
    "\n",
    "- data is **Independent and Identically Distributed** (iid)\n",
    "   - all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples\n",
    "   - identify cats and dogs on images\n",
    "   - predict the house price\n",
    "   - predict if someone's salary is above or below 50k\n",
    "- examples of not iid data (more on this next time):\n",
    "   - data generated by time-dependent processes\n",
    "   - data has group structure (samples collected from e.g., different subjects, experiments, measurement devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CV steps of iid data to avoid mistakes \n",
    "- shuffle and split the data\n",
    "- preprocess (fit_transform train, transform the rest)\n",
    "- decide on the evaluation metric\n",
    "- decide ML algo, which hyper-parameters you tune, and what values you want to try\n",
    "- loop over all combinations and save train and CV scores\n",
    "- find best model based on optimal CV score\n",
    "- report test score using the best model\n",
    "- repeat a couple of times with different random states to estimate uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='lightgray'>Cross Validation with iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- **apply simple CV and k-fold CV to datasets**\n",
    "- <font color='lightgray'>use GridSearchCV and pipelines</font>\n",
    "- <font color='lightgray'>apply stratified splits to imbalanced data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting strategies for iid data: basic approach\n",
    "- the basic aproach:\n",
    "   - 60% train, 20% CV, 20% test \n",
    "   - the ratios can vary somewhat but the training set should contain most of your points\n",
    "   - if you redo the split with a different random state, the results will change\n",
    "      - repeat the split a couple of times to measure model uncertainty due to splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's put everything together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "X = np.random.rand(n_samples)\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def ML_pipeline_basic(X,y,random_state):\n",
    "    # split the data\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state)\n",
    "    X_train, X_CV, y_train, y_CV = train_test_split(X_other, y_other, test_size=0.25, random_state = random_state)\n",
    "    # simple preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_CV = scaler.transform(X_CV)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # tune ridge hyper-parameter, alpha\n",
    "    alpha = np.logspace(-3,4,num=8)\n",
    "    train_score = []\n",
    "    CV_score = []\n",
    "    regs = []\n",
    "    for a in alpha:\n",
    "        reg = Ridge(alpha = a)\n",
    "        reg.fit(X_train,y_train)\n",
    "        train_score.append(mean_squared_error(y_train,reg.predict(X_train)))\n",
    "        CV_score.append(mean_squared_error(y_CV,reg.predict(X_CV)))\n",
    "        regs.append(reg)\n",
    "    # find the best alpha\n",
    "    best_alpha = alpha[np.argmin(CV_score)]\n",
    "    # grab the best model\n",
    "    reg = regs[np.argmin(CV_score)]\n",
    "    # calculate holdout score\n",
    "    test_score = mean_squared_error(y_test,reg.predict(X_test))\n",
    "    return best_alpha,np.min(CV_score),test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE: 0.18 +/- 0.04\n",
      "test MSE: 0.21 +/- 0.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "CV_scores = []\n",
    "test_scores = []\n",
    "for i in range(10):\n",
    "    best_alpha, CV_score, test_score = ML_pipeline_basic(X[:, np.newaxis],y, i*42 )\n",
    "    CV_scores.append(CV_score)\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "print('CV MSE:',np.around(np.mean(CV_scores),2),'+/-',np.around(np.std(CV_scores),2))\n",
    "print('test MSE:',np.around(np.mean(test_scores),2),'+/-',np.around(np.std(test_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "Add a couple of lines of code to `ML_pipeline_basic` to plot the train and the test scores as a function of alpha. Add x and y labels and also a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     plt.plot(alpha,train_score,label='train')\n",
    "#     plt.plot(alpha,test_score,label='test')\n",
    "#     plt.xlabel('alpha')\n",
    "#     plt.ylabel('MSE')\n",
    "#     plt.semilogx()\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting strategies for iid data, k-fold cross validation\n",
    "\n",
    "<center><img src=\"figures/grid_search_cross_validation.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why shuffling iid data is important?\n",
    "- by default, data is not shuffled by Kfold which can introduce errors!\n",
    "<center><img src=\"figures/kfold.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def ML_pipeline_kfold(X,y,random_state,n_folds):\n",
    "    # split the data\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state)\n",
    "    CV_scores = []\n",
    "    test_scores = []\n",
    "    # k folds - each fold will give us a CV and a test score\n",
    "    kf = KFold(n_splits=n_folds,shuffle=True,random_state=random_state)\n",
    "    for train_index, CV_index in kf.split(X_other,y_other):\n",
    "        X_train, X_CV = X_other[train_index], X_other[CV_index]\n",
    "        y_train, y_CV = y_other[train_index], y_other[CV_index]\n",
    "        # simple preprocessing\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_c = scaler.transform(X_CV)\n",
    "        X_t = scaler.transform(X_test)\n",
    "        # tune ridge hyper-parameter, alpha\n",
    "        alpha = np.logspace(-5,2,num=8)\n",
    "        train_score = []\n",
    "        CV_score = []\n",
    "        regs = []\n",
    "        for a in alpha:\n",
    "            reg = Ridge(alpha = a)\n",
    "            reg.fit(X_train,y_train)\n",
    "            train_score.append(mean_squared_error(y_train,reg.predict(X_train)))\n",
    "            CV_score.append(mean_squared_error(y_CV,reg.predict(X_c)))\n",
    "            regs.append(reg)\n",
    "        # find the best alpha in this fold\n",
    "        best_alpha = alpha[np.argmin(CV_score)]\n",
    "        # grab the best model\n",
    "        reg = regs[np.argmin(CV_score)]\n",
    "        CV_scores.append(np.min(CV_score))\n",
    "        # calculate test score using thee best model\n",
    "        test_scores.append(mean_squared_error(y_test,reg.predict(X_t)))\n",
    "    return CV_scores,test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE: 0.18 +/- 0.04\n",
      "test MSE: 0.163 +/- 0.003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "CV_scores, test_scores = ML_pipeline_kfold(X[:,np.newaxis],y,42,5)\n",
    "\n",
    "print('CV MSE:',np.around(np.mean(CV_scores),2),'+/-',np.around(np.std(CV_scores),2))\n",
    "print('test MSE:',np.around(np.mean(test_scores),3),'+/-',np.around(np.std(test_scores),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some considerations\n",
    "- 1) lots of lines of code were written, mistakes can be easily made!\n",
    "- 2) kfold CV uses the same test set, so we do not estimate the uncertainty from random test sets\n",
    "   - test score uncertainty is lower than in the basic approach\n",
    "- 3) both approaches (basic and kfold) can fail if the data is imbalanced\n",
    "   - if one class is infrequent, it can happen that one set or one fold contains 0 points from the rare class\n",
    "   - sklearn will raise an error in that case\n",
    "- 4) neither of these approaches work, if data is not iid!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='lightgray'>Cross Validation with iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='lightgray'>apply simple CV and k-fold CV to datasets</font>\n",
    "- **use GridSearchCV and pipelines**\n",
    "- <font color='lightgray'>apply stratified splits to imbalanced data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1) Let's shorten our code: GridSearchCV and pipeline in k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_pipeline_kfold(X,y,random_state,n_folds):\n",
    "    # create a test set\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state)\n",
    "    # splitter for _other\n",
    "    kf = KFold(n_splits=n_folds,shuffle=True,random_state=random_state)\n",
    "    # create the pipeline: preprocessor + supervised ML method\n",
    "    scaler = StandardScaler()\n",
    "    pipe = make_pipeline(scaler,Ridge())\n",
    "    # the parameter(s) we want to tune\n",
    "    param_grid = {'ridge__alpha': np.logspace(-3,4,num=8)}\n",
    "    # prepare gridsearch\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid,scoring = make_scorer(mean_squared_error,greater_is_better=False),\n",
    "                        cv=kf, return_train_score = True)\n",
    "    # do kfold CV on _other\n",
    "    grid.fit(X_other, y_other)\n",
    "    return grid, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE: 0.19 +/- 0.03\n",
      "test MSE: 0.16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_ridge__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'ridge__alpha': 0.001}</td>\n",
       "      <td>-0.202300</td>\n",
       "      <td>-0.157711</td>\n",
       "      <td>-0.168699</td>\n",
       "      <td>-0.160049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187654</td>\n",
       "      <td>0.034810</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.180344</td>\n",
       "      <td>-0.191596</td>\n",
       "      <td>-0.188761</td>\n",
       "      <td>-0.192012</td>\n",
       "      <td>-0.168994</td>\n",
       "      <td>-0.184341</td>\n",
       "      <td>0.008747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'ridge__alpha': 0.01}</td>\n",
       "      <td>-0.202294</td>\n",
       "      <td>-0.157719</td>\n",
       "      <td>-0.168705</td>\n",
       "      <td>-0.160029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187654</td>\n",
       "      <td>0.034814</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.180344</td>\n",
       "      <td>-0.191596</td>\n",
       "      <td>-0.188761</td>\n",
       "      <td>-0.192012</td>\n",
       "      <td>-0.168994</td>\n",
       "      <td>-0.184341</td>\n",
       "      <td>0.008747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'ridge__alpha': 0.1}</td>\n",
       "      <td>-0.202237</td>\n",
       "      <td>-0.157799</td>\n",
       "      <td>-0.168761</td>\n",
       "      <td>-0.159828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187650</td>\n",
       "      <td>0.034859</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.180344</td>\n",
       "      <td>-0.191597</td>\n",
       "      <td>-0.188762</td>\n",
       "      <td>-0.192013</td>\n",
       "      <td>-0.168995</td>\n",
       "      <td>-0.184342</td>\n",
       "      <td>0.008747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1</td>\n",
       "      <td>{'ridge__alpha': 1.0}</td>\n",
       "      <td>-0.201740</td>\n",
       "      <td>-0.158650</td>\n",
       "      <td>-0.169361</td>\n",
       "      <td>-0.157909</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187673</td>\n",
       "      <td>0.035312</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.180407</td>\n",
       "      <td>-0.191656</td>\n",
       "      <td>-0.188824</td>\n",
       "      <td>-0.192083</td>\n",
       "      <td>-0.169056</td>\n",
       "      <td>-0.184405</td>\n",
       "      <td>0.008748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>10</td>\n",
       "      <td>{'ridge__alpha': 10.0}</td>\n",
       "      <td>-0.202714</td>\n",
       "      <td>-0.170812</td>\n",
       "      <td>-0.178430</td>\n",
       "      <td>-0.145902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192327</td>\n",
       "      <td>0.040060</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.185235</td>\n",
       "      <td>-0.196205</td>\n",
       "      <td>-0.193658</td>\n",
       "      <td>-0.197511</td>\n",
       "      <td>-0.173772</td>\n",
       "      <td>-0.189276</td>\n",
       "      <td>0.008851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>100</td>\n",
       "      <td>{'ridge__alpha': 100.0}</td>\n",
       "      <td>-0.299603</td>\n",
       "      <td>-0.303486</td>\n",
       "      <td>-0.282612</td>\n",
       "      <td>-0.179732</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289302</td>\n",
       "      <td>0.064467</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.279937</td>\n",
       "      <td>-0.285426</td>\n",
       "      <td>-0.288467</td>\n",
       "      <td>-0.303964</td>\n",
       "      <td>-0.266270</td>\n",
       "      <td>-0.284813</td>\n",
       "      <td>0.012232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'ridge__alpha': 1000.0}</td>\n",
       "      <td>-0.454587</td>\n",
       "      <td>-0.475300</td>\n",
       "      <td>-0.419325</td>\n",
       "      <td>-0.279924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430692</td>\n",
       "      <td>0.082690</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.416955</td>\n",
       "      <td>-0.414514</td>\n",
       "      <td>-0.425639</td>\n",
       "      <td>-0.457985</td>\n",
       "      <td>-0.400098</td>\n",
       "      <td>-0.423038</td>\n",
       "      <td>0.019308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'ridge__alpha': 10000.0}</td>\n",
       "      <td>-0.486653</td>\n",
       "      <td>-0.509482</td>\n",
       "      <td>-0.446598</td>\n",
       "      <td>-0.302210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459481</td>\n",
       "      <td>0.085772</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.444815</td>\n",
       "      <td>-0.440761</td>\n",
       "      <td>-0.453530</td>\n",
       "      <td>-0.489302</td>\n",
       "      <td>-0.427309</td>\n",
       "      <td>-0.451143</td>\n",
       "      <td>0.020869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.000682      0.000119         0.000241        0.000034   \n",
       "1       0.000678      0.000067         0.000238        0.000024   \n",
       "2       0.000609      0.000006         0.000227        0.000002   \n",
       "3       0.000623      0.000019         0.000229        0.000002   \n",
       "4       0.000611      0.000012         0.000229        0.000007   \n",
       "5       0.000630      0.000054         0.000229        0.000014   \n",
       "6       0.000718      0.000172         0.000245        0.000050   \n",
       "7       0.000620      0.000024         0.000223        0.000002   \n",
       "\n",
       "  param_ridge__alpha                     params  split0_test_score  \\\n",
       "0              0.001    {'ridge__alpha': 0.001}          -0.202300   \n",
       "1               0.01     {'ridge__alpha': 0.01}          -0.202294   \n",
       "2                0.1      {'ridge__alpha': 0.1}          -0.202237   \n",
       "3                  1      {'ridge__alpha': 1.0}          -0.201740   \n",
       "4                 10     {'ridge__alpha': 10.0}          -0.202714   \n",
       "5                100    {'ridge__alpha': 100.0}          -0.299603   \n",
       "6               1000   {'ridge__alpha': 1000.0}          -0.454587   \n",
       "7              10000  {'ridge__alpha': 10000.0}          -0.486653   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0          -0.157711          -0.168699          -0.160049  ...   \n",
       "1          -0.157719          -0.168705          -0.160029  ...   \n",
       "2          -0.157799          -0.168761          -0.159828  ...   \n",
       "3          -0.158650          -0.169361          -0.157909  ...   \n",
       "4          -0.170812          -0.178430          -0.145902  ...   \n",
       "5          -0.303486          -0.282612          -0.179732  ...   \n",
       "6          -0.475300          -0.419325          -0.279924  ...   \n",
       "7          -0.509482          -0.446598          -0.302210  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0        -0.187654        0.034810                3           -0.180344   \n",
       "1        -0.187654        0.034814                2           -0.180344   \n",
       "2        -0.187650        0.034859                1           -0.180344   \n",
       "3        -0.187673        0.035312                4           -0.180407   \n",
       "4        -0.192327        0.040060                5           -0.185235   \n",
       "5        -0.289302        0.064467                6           -0.279937   \n",
       "6        -0.430692        0.082690                7           -0.416955   \n",
       "7        -0.459481        0.085772                8           -0.444815   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0           -0.191596           -0.188761           -0.192012   \n",
       "1           -0.191596           -0.188761           -0.192012   \n",
       "2           -0.191597           -0.188762           -0.192013   \n",
       "3           -0.191656           -0.188824           -0.192083   \n",
       "4           -0.196205           -0.193658           -0.197511   \n",
       "5           -0.285426           -0.288467           -0.303964   \n",
       "6           -0.414514           -0.425639           -0.457985   \n",
       "7           -0.440761           -0.453530           -0.489302   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0           -0.168994         -0.184341         0.008747  \n",
       "1           -0.168994         -0.184341         0.008747  \n",
       "2           -0.168995         -0.184342         0.008747  \n",
       "3           -0.169056         -0.184405         0.008748  \n",
       "4           -0.173772         -0.189276         0.008851  \n",
       "5           -0.266270         -0.284813         0.012232  \n",
       "6           -0.400098         -0.423038         0.019308  \n",
       "7           -0.427309         -0.451143         0.020869  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "grid, test_score = ML_pipeline_kfold(X[:,np.newaxis],y,42,5)\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print('CV MSE:',-np.around(results[results['rank_test_score'] == 1]['mean_test_score'].values[0],2),\\\n",
    "      '+/-',np.around(results[results['rank_test_score'] == 1]['std_test_score'].values[0],2))\n",
    "print('test MSE:',-np.around(test_score,2))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some notable differences\n",
    "- if multiple parameters give an equally good CV score, GridSearchCV returns the largest\n",
    "   - my function returns the smallest\n",
    "   - it's unclear which one is better\n",
    "- GridSearchCV calculates only one test score\n",
    "   - my function returns n_folds scores\n",
    "   - the new approach refits the best model to _other and that model is used to calculate the test score\n",
    "   - it's unclear which one is better\n",
    "      - my approach allows to calculate some uncertainty due to splitting (not on test)\n",
    "      - the GridSearchCV approach returns one test score but it is based on more data (likely more accurate)\n",
    "- 7 lines of code in GridSearchCV\n",
    "   - 28 lines of code in my function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2) Estimate the uncertainty from random test sets\n",
    "### Exercise 2 \n",
    "Calculate the test score for 10 different random splits. What's the mean and std test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE: 0.2 +/- 0.06\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "for i in range(10):\n",
    "    grid, test_score = ML_pipeline_kfold(X[:,np.newaxis],y,i*42,5)\n",
    "    test_scores.append(test_score)\n",
    "print('test MSE:',-np.around(np.mean(test_scores),2),'+/-',np.around(np.std(test_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='lightgray'>Cross Validation with iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='lightgray'>apply simple CV and k-fold CV to datasets</font>\n",
    "- <font color='lightgray'>use GridSearchCV and pipelines</font>\n",
    "- **apply stratified splits to imbalanced data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3) Imbalanced data: use stratified folds\n",
    "<center><img src=\"figures/stratified_kfold.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StratifiedKFold in module sklearn.model_selection._split:\n",
      "\n",
      "class StratifiedKFold(_BaseKFold)\n",
      " |  Stratified K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets.\n",
      " |  \n",
      " |  This cross-validation object is a variation of KFold that returns\n",
      " |  stratified folds. The folds are made by preserving the percentage of\n",
      " |  samples for each class.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=3\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          ``n_splits`` default value will change from 3 to 5 in v0.22.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle each class's samples before splitting into batches.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Used when ``shuffle`` == True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import StratifiedKFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([0, 0, 1, 1])\n",
      " |  >>> skf = StratifiedKFold(n_splits=2)\n",
      " |  >>> skf.get_n_splits(X, y)\n",
      " |  2\n",
      " |  >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in skf.split(X, y):\n",
      " |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [1 3] TEST: [0 2]\n",
      " |  TRAIN: [0 2] TEST: [1 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Train and test sizes may be different in each fold, with a difference of at\n",
      " |  most ``n_classes``.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StratifiedKFold\n",
      " |      _BaseKFold\n",
      " |      BaseCrossValidator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits='warn', shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  split(self, X, y, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |          Note that providing ``y`` is sufficient to generate the splits and\n",
      " |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      " |          ``X`` instead of actual training data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |          Stratification is done based on the y labels.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Randomized CV splitters may return different results for each call of\n",
      " |      split. You can make the results identical by setting ``random_state``\n",
      " |      to an integer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "help(StratifiedKFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now you can\n",
    "- apply simple CV and k-fold CV to datasets\n",
    "- use GridSearchCV with pipelines\n",
    "- apply stratified splits to imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
