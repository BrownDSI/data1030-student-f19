{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Go to piazza and open today's lecture notes in the hub! </center>\n",
    "## <center> https://piazza.com/class/jzioyk40mhs6r2 </center>\n",
    "## <center> Let's go to tophat for attendance! </center>\n",
    "## <center> https://app.tophat.com/e/245218 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **October 29:** Midterm exam\n",
    "   - **The first 10 lectures will be covered in the exam!**\n",
    "   - Please be here by 12:45 and start the hub\n",
    "   - Report any issues to Isabel Restrepo \n",
    "   - I will send out a github classroom invitation link around 12:45\n",
    "   - DO NOT click on the link before 1pm\n",
    "   - push your solution back to the repo and submit your pdf on gradescope by 2:20pm\n",
    "   - The next lecture starts here at 2:30pm, we need to be out by 2:25pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **October 31:** Guest lecturer August Guang from CCV\n",
    "   - **Is this a tipping point or am I just biased?**\n",
    "   - The term “tipping point” has become part of everyday English language, used to describe any kind of dramatic shift from which there is no return. However, actually defining and predicting a tipping point is extremely difficult, with the phenomenon often only becoming apparent post-hoc. Nevertheless, organizations make policy decisions based on the consequences of perceived tipping points. In light of this, we sought to understand: what characteristics make an individual more or less likely to declare a tipping point? Using logistic and random forest regression on survey data from 178 undergraduate and graduate students, we find that the perception of tipping points is primarily dependent on the characteristics of the graph, and secondarily on their own experience or emotions about tipping points. These conclusions have implications for management and sensemaking of perceived tipping points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mud card\n",
    "\n",
    "- **Can you give us more info about the test? how many questions? difficultly level compared to homework? how much time are we given?**\n",
    "   - 4 questions\n",
    "   - difficulty level is easier than the home work\n",
    "   - according to the TAs 1 hour should be enough to complete it so you have some buffer time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Mcar test**\n",
    "   - see lecture on September 24 for paper and 26 for code\n",
    "   - the mcar test takes a pandas dataframe of all numerical and np.nan values\n",
    "   - the null hypothesis is that the missingness pattern is consistent with MCAR\n",
    "      - if p value is large, null hypothesis is kept\n",
    "      - if p value is small (p < 0.05), null hypothesis is rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **missing data**\n",
    "   - see lecture on September 24 for code\n",
    "   - when you decide how to handle missing values, do three things first:\n",
    "      - calculate the mcar p value\n",
    "      - calculate what fraction of points have nans\n",
    "      - calculate the fraction of nans in each feature\n",
    "   - if p value is larger than 0.05 and only a few percent of points have missing values\n",
    "      - drop rows or do multivariate imputation\n",
    "   - if p value is small and/or a large fraction of points have missing values\n",
    "      - you cannot drop rows\n",
    "      - if multivariate imputation makes sense, impute\n",
    "      - if it does not, leave nans as is, we will cover that in November\n",
    "   - if one or a few features contain a large fraction of missing values (maybe 90% or above)\n",
    "      - if it make sense to drop the columns, do so\n",
    "      - if there are missing values left, repeat procedure without those features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **how to collect preprocessed features into a new data frame**\n",
    "   - if all features need to be preprocessed one way or another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all features\n",
    "cont_ftrs = []\n",
    "ordinal_ftrs = []\n",
    "ordinal_cats = []\n",
    "cat_ftrs = []\n",
    "\n",
    "# use the transformers and create new dataframes for each feature type\n",
    "df_cont\n",
    "df_ord\n",
    "df_cat\n",
    "# concatenate them\n",
    "df_prep = pd.concat([df_cont,df_ord,df_cat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "   - if only some features need to preprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ftrs = []\n",
    "# change the features in the original df and update the dataframe\n",
    "df[cat_ftrs] = scaled_feature_values # this is a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **when to make EDA figures with the original data versus the preprocessed data** \n",
    "   - tough question, it is case-specific\n",
    "   - continuous features: usually show original data\n",
    "   - categorical and ordinal features, it might make sense to show preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **What's the difference between ridge and lasso regression?**\n",
    "   - both are linear regression methods with regularization\n",
    "      - regularization can help you to avoid overfitting\n",
    "   - [lasso](https://scikit-learn.org/stable/modules/linear_model.html#lasso): regularization is done with the l1 norm of theta ($\\frac{\\alpha}{m} \\sum_{j=0}^{m}|\\theta_j|$)\n",
    "      - good for feature selection as well because if alpha is large, some thetas are 0\n",
    "   - [ridge](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression): regularization is done with the l2 norm of theta ($\\frac{\\alpha}{m} \\sum_{j=0}^{m} \\theta_j^2$)\n",
    "      - l2 regularization produces smoothly varying thetas as a function of alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Are there other cost function for logistic regression other than the log loss function?**\n",
    "   - not in sklearn but in principle you could construct other cost functions\n",
    "   - cost function inputs:\n",
    "      - the true values of the target variable\n",
    "      - the feature matrix\n",
    "      - model parameters\n",
    "   - cost function output:\n",
    "      - a single number that measures how well the model performs on the data given the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **In the first plot where we looked at the lasso regression, why did one of the features go to zero and then later on show up back again? how should this be interpreted?**\n",
    "   - that can happen\n",
    "   - it's OK because only one alpha value optimizies performance on the test set and you really only care about the corresponding theta coefficients\n",
    "   - in l2, some coefficients turn from negative to positive and vice versa.\n",
    "   \n",
    "<center><img src=\"figures/lasso_coefs.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"figures/ridge_coefs.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Can you go over bias and variance again, specifically what they mean in the context of the success of a model, and how they relate to metrics that we can look at, such as mse.**\n",
    "   - step 1: split your data to train and test\n",
    "   - step 2: choose an evaluation metric\n",
    "   - step 3: choose a model hyperparameter (e.g., the regularization parameter)\n",
    "   - step 4: loop through a wide range of hyperparameter values\n",
    "   - step 5: plot the tran and test scores\n",
    "   - high bias: model with a hyperparameter value performs poorly on both train and test\n",
    "   - high variace: model with a hyperparameter value perform very good on train but poorly on test\n",
    "      - model doesn't generalize to new points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/train_test_MSE_ridge.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **In logistic regression, what is penalty=‘l1’ and what is c=1/alpha**\n",
    "   - 'l1' is regularization using the l1 norm of theta\n",
    "   - 'l2' is regularization using the l2 norm of theta\n",
    "   - C is the inverse of the regularization parameter, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Can logistic regression be used for continuous target variables?**\n",
    "   - No. Logistic regression is a classification method. \n",
    "   - sklearn's logistic regression expects classification labels as the target variable (values between 0 and n_classes -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HW time!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
