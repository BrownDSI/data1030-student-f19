{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mud card\n",
    "- **Why do we use .score not .bestscore to calculate uncertainty in random state**\n",
    "   - .score() is a method of GridSearchCV and it takes a feature matrix and target variable as input and return the score based on the predictions of GridSearchCV's best model.\n",
    "      - .score() is used to calculate the test score **after** the model was fitted\n",
    "   - .best_score_ is an attribute of GridSearchCV and it returns the best CV score encountered while fitting\n",
    "- **Why not use randomforest for the \"seizure\" problem?**\n",
    "   - time constraints and I wanted to see how SVC performs\n",
    "   - in fact, I used XGBoost in the publication which is a tree-based method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Missing data, part 2\n",
    "\n",
    "By the end of this lecture, you will be able to\n",
    "- review simple approaches for handling missing values\n",
    "- Apply XGBoost to a dataset with missing values\n",
    "- Apply the reduced-features model (also called the pattern submodel approach)\n",
    "- Decide which approach is best for your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Missing values often occur in datasets\n",
    "- survey data: not everyone answers all the questions\n",
    "- medical data: not all tests/treatments/etc are performed on all patients\n",
    "- sensor can be offline or malfunctioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Missing values are an issue for multiple reasons\n",
    "\n",
    "#### Conceptual reason\n",
    "- missing values can introduce biases\n",
    "    - bias: the samples (the data points) are not representative of the underlying distribution/population\n",
    "    - any conclusion drawn from a biased dataset is also biased.\n",
    "    - rich people tend to not fill out survey questions about their salaries and the mean salary estimated from survey data tend to be lower than true value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Practical reason\n",
    "- missing values (NaN, NA, inf) are incompatible with sklearn\n",
    "   - all values in an array need to be numerical otherwise sklearn will throw a *ValueError*\n",
    "- there are a few supervised ML techniques that work with missing values (e.g., XGBoost)\n",
    "   - we will cover this later today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We work with the house price data set today\n",
    "- regression problem\n",
    "- categorical, ordinal, continuous features\n",
    "- missing data in all feature types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Missing data, part 2</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- **review simple approaches for handling missing values**\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple approaches for handling missing values\n",
    "- 1) categorical/ordinal features: treat missing values as another category\n",
    "    - missing values in categorical/ordinal features are not a big deal\n",
    "- 2) continuous features: this is the tough part\n",
    "    - sklearn's SimpleImputer\n",
    "- 3) exclude points or features with missing values\n",
    "    - might be OK\n",
    "- 4) multivariate imputation\n",
    "    - might be OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1a) Missing values in a categorical feature\n",
    "- YAY - this is not an issue at all!\n",
    "- Categorical feature needs to be one-hot encoded anyway\n",
    "- Just replace the missing values with 'NA' or 'missing' and treat it as a separate category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1b) Missing values in a ordinal feature\n",
    "- this can be a bit trickier but usually fine\n",
    "- Ordinal encoder is applied to ordinal features\n",
    "    - where does 'NA' or 'missing' fit into the order of the categories?\n",
    "    - usually first or last\n",
    "- if you can figure this out, you are fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 79)\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's load the data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "# drop the ID\n",
    "df.drop(columns=['Id'],inplace=True)\n",
    "\n",
    "# the target variable\n",
    "y = df['SalePrice']\n",
    "df.drop(columns=['SalePrice'],inplace=True)\n",
    "# the unprocessed feature matrix\n",
    "X = df.values\n",
    "print(X.shape)\n",
    "# the feature names\n",
    "ftrs = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 79)\n",
      "(292, 79)\n",
      "(292, 79)\n"
     ]
    }
   ],
   "source": [
    "# let's split to train, CV, and test\n",
    "X_other, X_test, y_other, y_test = train_test_split(df, y, test_size=0.2, random_state=0)\n",
    "X_train, X_CV, y_train, y_CV = train_test_split(X_other, y_other, test_size=0.25, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_CV.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# collect the various features\n",
    "cat_ftrs = ['MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood','Condition1','Condition2',\\\n",
    "            'BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation',\\\n",
    "           'Heating','CentralAir','Electrical','GarageType','PavedDrive','MiscFeature','SaleType','SaleCondition']\n",
    "ordinal_ftrs = ['LotShape','Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\\\n",
    "               'BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu','GarageFinish',\\\n",
    "               'GarageQual','GarageCond','PoolQC','Fence']\n",
    "ordinal_cats = [['Reg','IR1','IR2','IR3'],['AllPub','NoSewr','NoSeWa','ELO'],['Gtl','Mod','Sev'],\\\n",
    "               ['Po','Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],['NA','Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['NA','Po','Fa','TA','Gd','Ex'],['NA','No','Mn','Av','Gd'],['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\\\n",
    "               ['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],['Po','Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],['NA','Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['NA','Unf','RFn','Fin'],['NA','Po','Fa','TA','Gd','Ex'],['NA','Po','Fa','TA','Gd','Ex'],\n",
    "               ['NA','Fa','TA','Gd','Ex'],['NA','MnWw','GdWo','MnPrv','GdPrv']]\n",
    "num_ftrs = ['MSSubClass','LotFrontage','LotArea','OverallQual','OverallCond','YearBuilt','YearRemodAdd',\\\n",
    "             'MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF',\\\n",
    "             'LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr',\\\n",
    "             'KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars','GarageArea','WoodDeckSF',\\\n",
    "             'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','MoSold','YrSold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess with pipeline and columntransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# one-hot encoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "# ordinal encoder\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer2', SimpleImputer(strategy='constant',fill_value='NA')),\n",
    "    ('ordinal', OrdinalEncoder(categories = ordinal_cats))])\n",
    "\n",
    "# standard scaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# collect the transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs),\n",
    "        ('ord', ordinal_transformer, ordinal_ftrs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 221)\n",
      "(292, 221)\n",
      "(292, 221)\n"
     ]
    }
   ],
   "source": [
    "# fit_transform the training set\n",
    "X_prep = preprocessor.fit_transform(X_train)\n",
    "# little hacky, but collect feature names\n",
    "feature_names = preprocessor.transformers_[0][-1] + \\\n",
    "                list(preprocessor.named_transformers_['cat'][1].get_feature_names(cat_ftrs)) + \\\n",
    "                preprocessor.transformers_[2][-1]\n",
    "\n",
    "df_train = pd.DataFrame(data=X_prep,columns=feature_names)\n",
    "print(df_train.shape)\n",
    "\n",
    "# transform the CV\n",
    "df_CV = preprocessor.transform(X_CV)\n",
    "df_CV = pd.DataFrame(data=df_CV,columns = feature_names)\n",
    "print(df_CV.shape)\n",
    "\n",
    "# transform the test\n",
    "df_test = preprocessor.transform(X_test)\n",
    "df_test = pd.DataFrame(data=df_test,columns = feature_names)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2) Continuous features: mean or median imputation\n",
    "- Imputation means you infer the missing values from the known part of the data\n",
    "- sklearn's SimpleImputer can do mean and median imputation\n",
    "- USUALLY A BAD IDEA!\n",
    "   - MCAR: mean/median of non-missing values is the same as the mean/median of the true underlying distribution, but the variances are different\n",
    "   - not MCAR: the mean/median and the variance of the completed dataset will be off\n",
    "   - supervised ML model is too confident (MCAR) or systematically off (not MCAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3) Exclude points or features with missing values\n",
    "\n",
    "- easy to do with pandas\n",
    "- it is an ACCEPTABLE approach under two conditions:\n",
    "    - Little's test supports MCAR (p > 0.05)\n",
    "    - only small fraction of points contain missing values (maybe a few percent?)\n",
    "    - or the missing values are limited to one or a few features and a large fraction of values are missing from those features (maybe up to 90%?)\n",
    "- if the MCAR assumption is justified, dropping points will not introduce biases to your model\n",
    "- due to the smaller sample size, the confidence of your model might suffer. \n",
    "- what will you do with missing values when you deploy the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as ma\n",
    "import scipy.stats as st\n",
    "\n",
    "def checks_input_mcar_tests(data):\n",
    "    \"\"\" Checks whether the input parameter of class McarTests is correct\n",
    "            Parameters\n",
    "            ----------\n",
    "            data:\n",
    "                The input of McarTests specified as 'data'\n",
    "            Returns\n",
    "            -------\n",
    "            bool\n",
    "                True if input is correct\n",
    "            \"\"\"\n",
    "\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        print(\"Error: Data should be a Pandas DataFrame\")\n",
    "        return False\n",
    "\n",
    "    if not any(data.dtypes.values == np.float):\n",
    "        if not any(data.dtypes.values == np.int):\n",
    "            print(\"Error: Dataset cannot contain other value types than floats and/or integers\")\n",
    "            return False\n",
    "\n",
    "    if not data.isnull().values.any():\n",
    "        print(\"Error: No NaN's in given data\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def mcar_test(data):\n",
    "    \"\"\" Implementation of Little's MCAR test\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Pandas DataFrame\n",
    "        An incomplete dataset with samples as index and variables as columns\n",
    "    Returns\n",
    "    -------\n",
    "    p_value: Float\n",
    "        This value is the outcome of a chi-square statistical test, testing whether the null hypothesis\n",
    "        'the missingness mechanism of the incomplete dataset is MCAR' can be rejected.\n",
    "    \"\"\"\n",
    "\n",
    "    if not checks_input_mcar_tests(data):\n",
    "        raise Exception(\"Input not correct\")\n",
    "\n",
    "    dataset = data.copy()\n",
    "    vars = dataset.dtypes.index.values\n",
    "    n_var = dataset.shape[1]\n",
    "\n",
    "    # mean and covariance estimates\n",
    "    # ideally, this is done with a maximum likelihood estimator\n",
    "    gmean = dataset.mean()\n",
    "    gcov = dataset.cov()\n",
    "\n",
    "    # set up missing data patterns\n",
    "    r = 1 * dataset.isnull()\n",
    "    mdp = np.dot(r, list(map(lambda x: ma.pow(2, x), range(n_var))))\n",
    "    sorted_mdp = sorted(np.unique(mdp))\n",
    "    n_pat = len(sorted_mdp)\n",
    "    correct_mdp = list(map(lambda x: sorted_mdp.index(x), mdp))\n",
    "    dataset['mdp'] = pd.Series(correct_mdp, index=dataset.index)\n",
    "\n",
    "    # calculate statistic and df\n",
    "    pj = 0\n",
    "    d2 = 0\n",
    "    for i in range(n_pat):\n",
    "        dataset_temp = dataset.loc[dataset['mdp'] == i, vars]\n",
    "        select_vars = ~dataset_temp.isnull().any()\n",
    "        pj += np.sum(select_vars)\n",
    "        select_vars = vars[select_vars]\n",
    "        means = dataset_temp[select_vars].mean() - gmean[select_vars]\n",
    "        select_cov = gcov.loc[select_vars, select_vars]\n",
    "        mj = len(dataset_temp)\n",
    "        parta = np.dot(means.T, np.linalg.solve(select_cov, np.identity(select_cov.shape[1])))\n",
    "        d2 += mj * (np.dot(parta, means))\n",
    "\n",
    "    df = pj - n_var\n",
    "\n",
    "    # perform test and save output\n",
    "    p_value = 1 - st.chi2.cdf(d2, df)\n",
    "\n",
    "    return p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions: (876, 221)\n",
      "the p value of the mcar test: 0.029160269814447304\n",
      "fraction of missing values in features:\n",
      "LotFrontage    0.173516\n",
      "MasVnrArea     0.004566\n",
      "GarageYrBlt    0.050228\n",
      "dtype: float64\n",
      "fraction of points with missing values: 0.2237442922374429\n"
     ]
    }
   ],
   "source": [
    "print('data dimensions:',df_train.shape)\n",
    "print('the p value of the mcar test:',mcar_test(df_train))\n",
    "perc_missing_per_ftr = df_train.isnull().sum(axis=0)/df_train.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "frac_missing = sum(df_train.isnull().sum(axis=1)!=0)/df_train.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 221)\n",
      "(680, 221)\n",
      "(876, 218)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "# by default, rows/points are dropped\n",
    "df_r = df_train.dropna()\n",
    "print(df_r.shape)\n",
    "# drop features with missing values\n",
    "df_c = df_train.dropna(axis=1)\n",
    "print(df_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4) Multivariate Imputation\n",
    "\n",
    "- models each feature with missing values as a function of other features, and uses that estimate for imputation\n",
    "   - at each step, a feature is designated as target variable and the other feature columns are treated as feature matrix X\n",
    "   - a regressor is trained on (X, y) for known y\n",
    "   - then, the regressor is used to predict the missing values of y\n",
    "- in the ML pipeline:\n",
    "   - create n imputed datasets\n",
    "   - run all of them through the ML pipeline\n",
    "   - generate n test scores\n",
    "   - the uncertainty in the test scores is due to the uncertainty in imputation\n",
    "- works on MCAR and MAR, fails on MNAR\n",
    "- paper [here](https://www.jstatsoft.org/article/view/v045i03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sklearn's IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LotFrontage  MasVnrArea  GarageYrBlt\n",
      "0     0.424926   -0.573303     0.979398\n",
      "1          NaN    0.492835     1.018748\n",
      "2          NaN   -0.573303     0.192399\n",
      "3    -0.049970    0.810076    -0.476551\n",
      "4    -1.474659   -0.022031     0.979398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/sklearn/impute/_iterative.py:599: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LotFrontage  MasVnrArea  GarageYrBlt\n",
      "0     0.424926   -0.573303     0.979398\n",
      "1    -1.338235    0.492835     1.018748\n",
      "2    -0.277489   -0.573303     0.192399\n",
      "3    -0.049970    0.810076    -0.476551\n",
      "4    -1.474659   -0.022031     0.979398\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(df_train[['LotFrontage','MasVnrArea','GarageYrBlt']].head())\n",
    "\n",
    "imputer = IterativeImputer(estimator = RandomForestRegressor(n_estimators=100), random_state=1000)\n",
    "X_impute = imputer.fit_transform(df_train)\n",
    "df_train_imp = pd.DataFrame(data=X_impute, columns = df_train.columns)\n",
    "\n",
    "print(df_train_imp[['LotFrontage','MasVnrArea','GarageYrBlt']].head())\n",
    "\n",
    "df_CV_imp = pd.DataFrame(data=imputer.transform(df_CV), columns = df_train.columns)\n",
    "df_test_imp = pd.DataFrame(data=imputer.transform(df_test), columns = df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Missing data, part 2</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>review simple approaches for handling missing values</font>\n",
    "- **Apply XGBoost to a dataset with missing values**\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost\n",
    "- eXtreme Gradient Boosting - a popular tree-based method\n",
    "- [blog post](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) and [paper](http://delivery.acm.org/10.1145/2940000/2939785/p785-chen.pdf)\n",
    "- more advanced than random forest\n",
    "   - it has l1 and l2 regularization while random forest does not\n",
    "   - trees are not independent\n",
    "      - the next tree is built to improve the previous tree\n",
    "      - less trees are necessary to achieve same accuracy\n",
    "      - but XGBoost trees can overfit\n",
    "   - handles missing values well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost and missing values\n",
    "- sklearn raises an error if the feature matrix (X) contains nans. \n",
    "- XGBoost doesn't! \n",
    "- If a feature with missing values is split:\n",
    "    - XGBoost tries to put the points with missing values to the left and right\n",
    "    - calculates the impurity measure for both options\n",
    "    - puts the points with missing values to the side with the lower impurity\n",
    "- if missingness correlates with the target variable, XGBoost extracts this info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the CV RMSE: 24197.591051247382\n",
      "the test RMSE: 33267.239641714295\n",
      "the test R2: 0.8397432265183982\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "param_grid = {\"learning_rate\": [0.03],\n",
    "              \"n_estimators\": [2000],\n",
    "              \"seed\": [0],\n",
    "              #\"reg_alpha\": [0e0,0.01,0.1,1.,10.,100],\n",
    "              #\"reg_lambda\": [0e0,0.01,0.1,1.,10.,100],\n",
    "              \"missing\": [np.nan], \n",
    "              #\"max_depth\": [1,3,10,30,100],\n",
    "              \"colsample_bytree\": [0.9],              \n",
    "              \"subsample\": [0.66]}\n",
    "\n",
    "XGB = xgboost.XGBRegressor()\n",
    "XGB.set_params(**ParameterGrid(param_grid)[0])\n",
    "XGB.fit(df_train,y_train,early_stopping_rounds=50,eval_set=[(df_CV, y_CV)], verbose=False)\n",
    "y_CV_pred = XGB.predict(df_CV)\n",
    "print('the CV RMSE:',np.sqrt(mean_squared_error(y_CV,y_CV_pred)))\n",
    "y_test_pred = XGB.predict(df_test)\n",
    "print('the test RMSE:',np.sqrt(mean_squared_error(y_test,y_test_pred)))\n",
    "print('the test R2:',r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### XGBoost with the imputed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the CV RMSE: 24615.999556938234\n",
      "the test RMSE: 33798.2594122664\n",
      "the test R2: 0.8345862789876468\n"
     ]
    }
   ],
   "source": [
    "XGB.fit(df_train_imp,y_train,early_stopping_rounds=50,eval_set=[(df_CV_imp, y_CV)], verbose=False)\n",
    "y_CV_pred = XGB.predict(df_CV_imp)\n",
    "print('the CV RMSE:',np.sqrt(mean_squared_error(y_CV,y_CV_pred)))\n",
    "y_test_pred = XGB.predict(df_test_imp)\n",
    "print('the test RMSE:',np.sqrt(mean_squared_error(y_test,y_test_pred)))\n",
    "print('the test R2:',r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Missing data, part 2</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>review simple approaches for handling missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- **Apply the reduced-features model (also called the pattern submodel approach)**\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reduced-features model (or pattern submodel approach)\n",
    "- first described in 2007 in a [JMLR article](http://www.jmlr.org/papers/v8/saar-tsechansky07a.html) as the reduced features model\n",
    "- in 2018, \"rediscovered\" as the pattern submodel approach in [Biostatistics](https://www.ncbi.nlm.nih.gov/pubmed/30203058)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>My test set:</center>\n",
    "\n",
    "| index \t| feature 1 \t| feature 2 \t| feature 3 \t| target var \t|\n",
    "|-------\t|:---------:\t|:---------:\t|:---------:\t|:----------:\t|\n",
    "| 0     \t|     <font color='red'>NA</font>    \t|     45    \t|     <font color='red'>NA</font>    \t|      0     \t|\n",
    "| 1     \t|     <font color='red'>NA</font>    \t|     <font color='red'>NA</font>    \t|     8     \t|      1     \t|\n",
    "| 2     \t|     12    \t|     6     \t|     34    \t|      0     \t|\n",
    "| 3     \t|     1     \t|     89    \t|     <font color='red'>NA</font>    \t|      0     \t|\n",
    "| 4     \t|     0     \t|     <font color='red'>NA</font>    \t|     47    \t|      1     \t|\n",
    "| 5     \t|    687    \t|     24    \t|     67    \t|      1     \t|\n",
    "| 6     \t|     <font color='red'>NA</font>    \t|     23    \t|     <font color='red'>NA</font>    \t|      1     \t|\n",
    "\n",
    "To predict points 0 and 6, I will use train and CV points that are complete in feature 2.\n",
    "\n",
    "To predict point 1, I will use train and CV points that are complete in feature 3.\n",
    "\n",
    "To predict point 2 and 5, I will use train and CV points that are complete in features 1-3.\n",
    "\n",
    "Etc. We will train as many models as the number of patterns in test/deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to determine the patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3)\n",
      "[False False False] 223\n",
      "[False False  True] 21\n",
      "[False  True False] 1\n",
      "[ True False False] 44\n",
      "[ True False  True] 2\n",
      "[ True  True False] 1\n"
     ]
    }
   ],
   "source": [
    "mask = df_test[['LotFrontage','MasVnrArea','GarageYrBlt']].isnull()\n",
    "unique_rows, counts = np.unique(mask, axis=0,return_counts=True)\n",
    "print(unique_rows.shape) # 6 patterns, we will train 6 models\n",
    "for i in range(len(counts)):\n",
    "    print(unique_rows[i],counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def xgb_model(X_train, Y_train, X_CV, y_CV, X_test, y_test, verbose=1):\n",
    "\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    Y_train = np.reshape(np.array(Y_train), (1, -1)).ravel()\n",
    "    y_CV = np.reshape(np.array(y_CV), (1, -1)).ravel()\n",
    "    y_test = np.reshape(np.array(y_test), (1, -1)).ravel()\n",
    "\n",
    "    XGB = xgboost.XGBRegressor(n_jobs=1)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {\"learning_rate\": [0.03],\n",
    "                  \"n_estimators\": [2000],\n",
    "                  \"seed\": [0],\n",
    "                  #\"reg_alpha\": [0e0,0.1,0.31622777,1.,3.16227766,10.],\n",
    "                  #\"reg_lambda\": [0e0,0.1,0.31622777,1.,3.16227766,10.],\n",
    "                  \"missing\": [np.nan], \n",
    "                  #\"max_depth\": [1,2,3,4,5],\n",
    "                  \"colsample_bytree\": [0.9],              \n",
    "                  \"subsample\": [0.66]}\n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros(len(pg))\n",
    "\n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "            print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        XGB.set_params(**params)\n",
    "        eval_set = [(X_CV, y_CV)]\n",
    "        XGB.fit(X_train, Y_train,\n",
    "                early_stopping_rounds=50, eval_set=eval_set, verbose=False)# with early stopping\n",
    "        y_CV_pred = XGB.predict(X_CV, ntree_limit=XGB.best_ntree_limit)\n",
    "        scores[i] = mean_squared_error(y_CV,y_CV_pred)\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    if verbose >= 4:\n",
    "        print('Test set max score and best parameters are:')\n",
    "        print(np.max(scores))\n",
    "        print(best_params)\n",
    "\n",
    "    # test the model on the test set with best parameter set\n",
    "    XGB.set_params(**best_params[0])\n",
    "    XGB.fit(X_train,Y_train,\n",
    "            early_stopping_rounds=50,eval_set=eval_set, verbose=False)\n",
    "    y_test_pred = XGB.predict(X_test, ntree_limit=XGB.best_ntree_limit)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The MSE is:',mean_squared_error(y_test,y_test_pred))\n",
    "    if verbose >= 2:\n",
    "        print ('The predictions are:')\n",
    "        print (y_test_pred)\n",
    "    if verbose >= 3:\n",
    "        print(\"Feature importances:\")\n",
    "        print(XGB.feature_importances_)\n",
    "\n",
    "    return (mean_squared_error(y_test,y_test_pred), y_test_pred, XGB.feature_importances_)\n",
    "\n",
    "# Function: Reduced-feature XGB model\n",
    "# all the inputs need to be pandas DataFrame\n",
    "def reduced_feature_xgb(X_train, Y_train, X_CV, y_CV, X_test, y_test):\n",
    "    \n",
    "    # find all unique patterns of missing value in test set\n",
    "    mask = X_test.isnull()\n",
    "    unique_rows = np.array(np.unique(mask, axis=0))\n",
    "    all_y_test_pred = pd.DataFrame()\n",
    "    \n",
    "    print('there are', len(unique_rows), 'unique missing value patterns.')\n",
    "    \n",
    "    # divide test sets into subgroups according to the unique patterns\n",
    "    for i in range(len(unique_rows)):\n",
    "        print ('working on unique pattern', i)\n",
    "        ## generate X_test subset that matches the unique pattern i\n",
    "        sub_X_test = pd.DataFrame()\n",
    "        sub_y_test = pd.Series()\n",
    "        for j in range(len(mask)): # check each row in mask\n",
    "            row_mask = np.array(mask.iloc[j])\n",
    "            if np.array_equal(row_mask, unique_rows[i]): # if the pattern matches the ith unique pattern\n",
    "                sub_X_test = sub_X_test.append(X_test.iloc[j])# append the according X_test row j to the subset\n",
    "                sub_y_test = sub_y_test.append(y_test.iloc[[j]])# append the according y_test row j\n",
    "        sub_X_test = sub_X_test[X_test.columns[~unique_rows[i]]]\n",
    "        \n",
    "        ## choose the according reduced features for subgroups\n",
    "        sub_X_train = pd.DataFrame()\n",
    "        sub_Y_train = pd.DataFrame()\n",
    "        sub_X_CV = pd.DataFrame()\n",
    "        sub_y_CV = pd.DataFrame()\n",
    "        # 1.cut the feature columns that have nans in the according sub_X_test\n",
    "        sub_X_train = X_train[X_train.columns[~unique_rows[i]]]\n",
    "        sub_X_CV = X_CV[X_CV.columns[~unique_rows[i]]]\n",
    "        # 2.cut the rows in the sub_X_train and sub_X_CV that have any nans\n",
    "        sub_X_train = sub_X_train.dropna()\n",
    "        sub_X_CV = sub_X_CV.dropna()   \n",
    "        # 3.cut the sub_Y_train and sub_y_CV accordingly\n",
    "        sub_Y_train = Y_train.iloc[sub_X_train.index]\n",
    "        sub_y_CV = y_CV.iloc[sub_X_CV.index]\n",
    "        \n",
    "        # run XGB\n",
    "        sub_y_test_pred = xgb_model(sub_X_train, sub_Y_train, sub_X_CV, \n",
    "                                       sub_y_CV, sub_X_test, sub_y_test, verbose=0)\n",
    "        sub_y_test_pred = pd.DataFrame(sub_y_test_pred[1],columns=['sub_y_test_pred'],\n",
    "                                          index=sub_y_test.index)\n",
    "        print('   RMSE:',np.sqrt(mean_squared_error(sub_y_test,sub_y_test_pred)))\n",
    "        # collect the test predictions\n",
    "        all_y_test_pred = all_y_test_pred.append(sub_y_test_pred)\n",
    "        \n",
    "    # rank the final y_test_pred according to original y_test index\n",
    "    all_y_test_pred = all_y_test_pred.sort_index()\n",
    "    y_test = y_test.sort_index()\n",
    "               \n",
    "    # get global RMSE\n",
    "    total_RMSE = np.sqrt(mean_squared_error(y_test,all_y_test_pred))\n",
    "    total_R2 =  r2_score(y_test,all_y_test_pred)\n",
    "    return total_RMSE, total_R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A python implementation is available on the skipped slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6 unique missing value patterns.\n",
      "working on unique pattern 0\n",
      "   RMSE: 36882.33649761256\n",
      "working on unique pattern 1\n",
      "   RMSE: 14122.835714181436\n",
      "working on unique pattern 2\n",
      "   RMSE: 7912.15625\n",
      "working on unique pattern 3\n",
      "   RMSE: 19059.686269128022\n",
      "working on unique pattern 4\n",
      "   RMSE: 20818.556654658543\n",
      "working on unique pattern 5\n",
      "   RMSE: 55023.453125\n",
      "final RMSE: 33488.799390845474\n",
      "final R2: 0.8376014986589545\n"
     ]
    }
   ],
   "source": [
    "RMSE, R2 = reduced_feature_xgb(df_train, y_train, df_CV, y_CV, df_test, y_test)\n",
    "print('final RMSE:', RMSE)\n",
    "print('final R2:', R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Missing data, part 2</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>review simple approaches for handling missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- **Decide which approach is best for your dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which approach is best for my data?\n",
    "- **XGB**: run $n$ XGB models with $n$ different seeds\n",
    "- **imputation**: prepare $n$ different imputations and run $n$ XGB models on them\n",
    "- **reduced-features**: run $n$ reduced-features model with $n$ different seeds\n",
    "- rank the three methods based on how significantly different the corresponding mean scores are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now you can\n",
    "- review simple approaches for handling missing values\n",
    "- Apply XGBoost to a dataset with missing values\n",
    "- Apply the reduced-features model (also called the pattern submodel approach)\n",
    "- Decide which approach is best for your dataset"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
