{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Go to piazza and open today's lecture notes in the hub! </center>\n",
    "## <center> https://piazza.com/class/jzioyk40mhs6r2 </center>\n",
    "## <center> Let's go to tophat for attendance! </center>\n",
    "## <center> https://app.tophat.com/e/245218 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project\n",
    "- hopefully your TA mentor already sent an email to you.\n",
    "- advice on project:\n",
    "   - don't choose a very well-known dataset! No iris, titanic, MNIST, etc.\n",
    "   - kaggle dataset is OK but don't participate in a competition.\n",
    "      - use the kaggle dataset to answer a different question.\n",
    "   - UCI ML repository is a great resource for datasets\n",
    "   - supervised ML problem is preferred.\n",
    "- send thee first draft of the project description to the TA mentor  by the 24th of September.\n",
    "- send the final project description to me by the **30th of September**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Mud card questions </center>\n",
    "\n",
    "- **Pandas merge and sql join, same thing?**\n",
    "   - Yes!\n",
    "- **Is it possible to represent all kinds of merges through combinations of left, right, inner, and outer joins?**\n",
    "   - I don't think so. Sometimes you need pd.concat to do more complex combinations.\n",
    "- **When you merge, how are the values sorted ? or are they not sorted?**\n",
    "   -  depends on the type of join you use and whether sort is True or False. Check the manual and experiment with a toy dataset!\n",
    "- **I understand when to use brackets vs paren, but it's a little unclear that [[ ]] seems to indicate to iloc that you are now referencing multiple rows separated by commas**\n",
    "   - The inner [] is a list of indices which is passed to .iloc[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Data preprocessing, part 1, categorical and continuous features </center>\n",
    "### By the end of this course, you will be able to\n",
    "- describe why preprocessing is necessary\n",
    "- apply one-hot encoding or ordinal encoding to categorical variables\n",
    "- apply scaling and normalization to continuous variables\n",
    "- apply label encoding to a categorical target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this course, you will be able to</font>\n",
    "- **describe why preprocessing is necessary**\n",
    "- <font color='LIGHTGRAY'>apply one-hot encoding or ordinal encoding to categorical variables</font>\n",
    "- <font color='LIGHTGRAY'>apply scaling and normalization to continuous variables</font>\n",
    "- <font color='LIGHTGRAY'>apply label encoding to a categorical target variable</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem description, why preprocessing is necessary\n",
    "\n",
    "Data format suitable for ML: 2D numerical values.\n",
    "\n",
    "| X|feature_1|feature_2|...|feature_j|...|feature_m|<font color='red'>y</font>|\n",
    "|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__data_point_1__|x_11|x_12|...|x_1j|...|x_1m|__<font color='red'>y_1</font>__|\n",
    "|__data_point_2__|x_21|x_22|...|x_2j|...|x_2m|__<font color='red'>y_2</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>__|\n",
    "|__data_point_i__|x_i1|x_i2|...|x_ij|...|x_im|__<font color='red'>y_i</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>__|\n",
    "|__data_point_n__|x_n1|x_n2|...|x_nj|...|x_nm|__<font color='red'>y_n</font>__|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data almost never comes in a format that's directly usable in ML.\n",
    "- ML works with numerical data but some columns can be text (e.g., home country, educational level, gender, race)\n",
    "- the order of magnitude of numerical features can vary greatly which is not good for most ML algorithms (e.g., salary in USD, age in years, time spent on the site in sec)\n",
    "- the target variable is not in the right format (e.g., the target variable is text in classification (<=50K, >50K))\n",
    "- some values are missing and ML methods implemented in scikit-learn don't work with NaNs - next lecture will cover this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### scikit-learn transformers to the rescue!\n",
    "\n",
    "Preprocessing is done with various transformers. All transformes have three methods:\n",
    "- **fit** method: estimates parameters necessary to do the transformation,\n",
    "- **transform** method: transforms the data based on the estimated parameters,\n",
    "- **fit_transform** method: both steps are performed at once, this can be faster than doing the steps separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformers we cover today\n",
    "- **OneHotEncoder** - converts categorical features into dummy arrays\n",
    "- **OrdinalEncoder** - converts categorical features into an integer array\n",
    "- **MinMaxScaler** - scales continuous variables to be between 0 and 1\n",
    "- **StandardScaler** - standardizes continuous features by removing the mean and scaling to unit variance\n",
    "- **LabelEncoder** - converts text target variable to numerical values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this course, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>describe why preprocessing is necessary</font>\n",
    "- **apply one-hot encoding or ordinal encoding to categorical variables**\n",
    "- <font color='LIGHTGRAY'>apply scaling and normalization to continuous variables</font>\n",
    "- <font color='LIGHTGRAY'>apply label encoding to a categorical target variable</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ordered categorical data: OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a categorical feature and a training and test sets\n",
    "\n",
    "The cateogies can be ordered or ranked\n",
    "\n",
    "E.g., educational level in the adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_edu = {'educational level':['Bachelors','Masters','Bachelors','Doctorate','HS-grad','Masters']} \n",
    "test_edu = {'educational level':['HS-grad','Masters','Masters','College','Bachelors']}\n",
    "\n",
    "X_train = pd.DataFrame(train_edu)\n",
    "X_test = pd.DataFrame(test_edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OrdinalEncoder in module sklearn.preprocessing._encoders:\n",
      "\n",
      "class OrdinalEncoder(_BaseEncoder)\n",
      " |  Encode categorical features as an integer array.\n",
      " |  \n",
      " |  The input to this transformer should be an array-like of integers or\n",
      " |  strings, denoting the values taken on by categorical (discrete) features.\n",
      " |  The features are converted to ordinal integers. This results in\n",
      " |  a single column of integers (0 to n_categories - 1) per feature.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  categories : 'auto' or a list of lists/arrays of values.\n",
      " |      Categories (unique values) per feature:\n",
      " |  \n",
      " |      - 'auto' : Determine categories automatically from the training data.\n",
      " |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      " |        column. The passed categories should not mix strings and numeric\n",
      " |        values, and should be sorted in case of numeric values.\n",
      " |  \n",
      " |      The used categories can be found in the ``categories_`` attribute.\n",
      " |  \n",
      " |  dtype : number type, default np.float64\n",
      " |      Desired dtype of output.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  categories_ : list of arrays\n",
      " |      The categories of each feature determined during fitting\n",
      " |      (in order of the features in X and corresponding with the output\n",
      " |      of ``transform``).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Given a dataset with two features, we let the encoder find the unique\n",
      " |  values per feature and transform the data to an ordinal encoding.\n",
      " |  \n",
      " |  >>> from sklearn.preprocessing import OrdinalEncoder\n",
      " |  >>> enc = OrdinalEncoder()\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      " |  >>> enc.fit(X)\n",
      " |  ... # doctest: +ELLIPSIS\n",
      " |  OrdinalEncoder(categories='auto', dtype=<... 'numpy.float64'>)\n",
      " |  >>> enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> enc.transform([['Female', 3], ['Male', 1]])\n",
      " |  array([[0., 2.],\n",
      " |         [1., 0.]])\n",
      " |  \n",
      " |  >>> enc.inverse_transform([[1, 0], [0, 1]])\n",
      " |  array([['Male', 1],\n",
      " |         ['Female', 2]], dtype=object)\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
      " |    categorical features.\n",
      " |  sklearn.preprocessing.LabelEncoder : encodes target labels with values\n",
      " |    between 0 and n_classes-1.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OrdinalEncoder\n",
      " |      _BaseEncoder\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, categories='auto', dtype=<class 'numpy.float64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the OrdinalEncoder to X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to determine the categories of each feature.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the data back to the original representation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n",
      " |          The transformed data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : array-like, shape [n_samples, n_features]\n",
      " |          Inverse transformed array.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X to ordinal codes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : sparse matrix or a 2-d array\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "help(OrdinalEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['HS-grad', 'Bachelors', 'Masters', 'Doctorate'], dtype=object)]\n",
      "[[1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['College'] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b58c0d5032ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_oe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# transform X_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mX_test_oe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# OrdinalEncoder always throws an error message if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                                   \u001b[0;31m# it encounters an unknown category in test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \"\"\"\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    120\u001b[0m                     msg = (\"Found unknown categories {0} in column {1}\"\n\u001b[1;32m    121\u001b[0m                            \" during transform\".format(diff, i))\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# Set the problematic rows to an acceptable value and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories ['College'] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "# initialize the encoder\n",
    "enc = OrdinalEncoder(categories = [['HS-grad','Bachelors','Masters','Doctorate']]) # The ordered list of \n",
    "# categories need to be provided. By default, the categories are alphabetically ordered!\n",
    "\n",
    "# fit the training data\n",
    "enc.fit(X_train)\n",
    "# print the categories - not really important because we manually gave the ordered list of categories\n",
    "print(enc.categories_)\n",
    "# transform X_train. We could have used enc.fit_transform(X_train) to combine fit and transform\n",
    "X_train_oe = enc.transform(X_train)\n",
    "print(X_train_oe)\n",
    "# transform X_test\n",
    "X_test_oe = enc.transform(X_test) # OrdinalEncoder always throws an error message if \n",
    "                                  # it encounters an unknown category in test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unordered categorical data: one-hot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some categories cannot be ordered. e.g., workclass, relationship status\n",
    "\n",
    "first feature: gender (male, female, unknown)\n",
    "\n",
    "second feature: browser  used \n",
    "\n",
    "these categories cannot be ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {'gender':['Male','Female','Unknown','Male','Female','Female'],\\\n",
    "         'browser':['Safari','Safari','Internet Explorer','Chrome','Chrome','Internet Explorer']}\n",
    "test = {'gender':['Female','Male','Unknown','Female'],'browser':['Chrome','Firefox','Internet Explorer','Safari']}\n",
    "\n",
    "X_train = pd.DataFrame(train)\n",
    "X_test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OneHotEncoder in module sklearn.preprocessing._encoders:\n",
      "\n",
      "class OneHotEncoder(_BaseEncoder)\n",
      " |  Encode categorical integer features as a one-hot numeric array.\n",
      " |  \n",
      " |  The input to this transformer should be an array-like of integers or\n",
      " |  strings, denoting the values taken on by categorical (discrete) features.\n",
      " |  The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n",
      " |  encoding scheme. This creates a binary column for each category and\n",
      " |  returns a sparse matrix or dense array.\n",
      " |  \n",
      " |  By default, the encoder derives the categories based on the unique values\n",
      " |  in each feature. Alternatively, you can also specify the `categories`\n",
      " |  manually.\n",
      " |  The OneHotEncoder previously assumed that the input features take on\n",
      " |  values in the range [0, max(values)). This behaviour is deprecated.\n",
      " |  \n",
      " |  This encoding is needed for feeding categorical data to many scikit-learn\n",
      " |  estimators, notably linear models and SVMs with the standard kernels.\n",
      " |  \n",
      " |  Note: a one-hot encoding of y labels should use a LabelBinarizer\n",
      " |  instead.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  categories : 'auto' or a list of lists/arrays of values, default='auto'.\n",
      " |      Categories (unique values) per feature:\n",
      " |  \n",
      " |      - 'auto' : Determine categories automatically from the training data.\n",
      " |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      " |        column. The passed categories should not mix strings and numeric\n",
      " |        values within a single feature, and should be sorted in case of\n",
      " |        numeric values.\n",
      " |  \n",
      " |      The used categories can be found in the ``categories_`` attribute.\n",
      " |  \n",
      " |  drop : 'first' or a list/array of shape (n_features,), default=None.\n",
      " |      Specifies a methodology to use to drop one of the categories per\n",
      " |      feature. This is useful in situations where perfectly collinear\n",
      " |      features cause problems, such as when feeding the resulting data\n",
      " |      into a neural network or an unregularized regression.\n",
      " |  \n",
      " |      - None : retain all features (the default).\n",
      " |      - 'first' : drop the first category in each feature. If only one\n",
      " |        category is present, the feature will be dropped entirely.\n",
      " |      - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n",
      " |        should be dropped.\n",
      " |  \n",
      " |  sparse : boolean, default=True\n",
      " |      Will return sparse matrix if set True else will return an array.\n",
      " |  \n",
      " |  dtype : number type, default=np.float\n",
      " |      Desired dtype of output.\n",
      " |  \n",
      " |  handle_unknown : 'error' or 'ignore', default='error'.\n",
      " |      Whether to raise an error or ignore if an unknown categorical feature\n",
      " |      is present during transform (default is to raise). When this parameter\n",
      " |      is set to 'ignore' and an unknown category is encountered during\n",
      " |      transform, the resulting one-hot encoded columns for this feature\n",
      " |      will be all zeros. In the inverse transform, an unknown category\n",
      " |      will be denoted as None.\n",
      " |  \n",
      " |  n_values : 'auto', int or array of ints, default='auto'\n",
      " |      Number of values per feature.\n",
      " |  \n",
      " |      - 'auto' : determine value range from training data.\n",
      " |      - int : number of categorical values per feature.\n",
      " |              Each feature value should be in ``range(n_values)``\n",
      " |      - array : ``n_values[i]`` is the number of categorical values in\n",
      " |                ``X[:, i]``. Each feature value should be\n",
      " |                in ``range(n_values[i])``\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |          The `n_values` keyword was deprecated in version 0.20 and will\n",
      " |          be removed in 0.22. Use `categories` instead.\n",
      " |  \n",
      " |  categorical_features : 'all' or array of indices or mask, default='all'\n",
      " |      Specify what features are treated as categorical.\n",
      " |  \n",
      " |      - 'all': All features are treated as categorical.\n",
      " |      - array of indices: Array of categorical feature indices.\n",
      " |      - mask: Array of length n_features and with dtype=bool.\n",
      " |  \n",
      " |      Non-categorical features are always stacked to the right of the matrix.\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |          The `categorical_features` keyword was deprecated in version\n",
      " |          0.20 and will be removed in 0.22.\n",
      " |          You can use the ``ColumnTransformer`` instead.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  categories_ : list of arrays\n",
      " |      The categories of each feature determined during fitting\n",
      " |      (in order of the features in X and corresponding with the output\n",
      " |      of ``transform``). This includes the category specified in ``drop``\n",
      " |      (if any).\n",
      " |  \n",
      " |  drop_idx_ : array of shape (n_features,)\n",
      " |      ``drop_idx_[i]`` isÂ the index in ``categories_[i]`` of the category to\n",
      " |      be dropped for each feature. None if all the transformed features will\n",
      " |      be retained.\n",
      " |  \n",
      " |  active_features_ : array\n",
      " |      Indices for active features, meaning values that actually occur\n",
      " |      in the training set. Only available when n_values is ``'auto'``.\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |          The ``active_features_`` attribute was deprecated in version\n",
      " |          0.20 and will be removed in 0.22.\n",
      " |  \n",
      " |  feature_indices_ : array of shape (n_features,)\n",
      " |      Indices to feature ranges.\n",
      " |      Feature ``i`` in the original data is mapped to features\n",
      " |      from ``feature_indices_[i]`` to ``feature_indices_[i+1]``\n",
      " |      (and then potentially masked by ``active_features_`` afterwards)\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |          The ``feature_indices_`` attribute was deprecated in version\n",
      " |          0.20 and will be removed in 0.22.\n",
      " |  \n",
      " |  n_values_ : array of shape (n_features,)\n",
      " |      Maximum number of values per feature.\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |          The ``n_values_`` attribute was deprecated in version\n",
      " |          0.20 and will be removed in 0.22.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Given a dataset with two features, we let the encoder find the unique\n",
      " |  values per feature and transform the data to a binary one-hot encoding.\n",
      " |  \n",
      " |  >>> from sklearn.preprocessing import OneHotEncoder\n",
      " |  >>> enc = OneHotEncoder(handle_unknown='ignore')\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      " |  >>> enc.fit(X)\n",
      " |  ... # doctest: +ELLIPSIS\n",
      " |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      " |  OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
      " |     dtype=<... 'numpy.float64'>, handle_unknown='ignore',\n",
      " |     n_values=None, sparse=True)\n",
      " |  \n",
      " |  >>> enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
      " |  array([[1., 0., 1., 0., 0.],\n",
      " |         [0., 1., 0., 0., 0.]])\n",
      " |  >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
      " |  array([['Male', 1],\n",
      " |         [None, 2]], dtype=object)\n",
      " |  >>> enc.get_feature_names()\n",
      " |  array(['x0_Female', 'x0_Male', 'x1_1', 'x1_2', 'x1_3'], dtype=object)\n",
      " |  >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n",
      " |  >>> drop_enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      " |  array([[0., 0., 0.],\n",
      " |         [1., 1., 0.]])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.preprocessing.OrdinalEncoder : performs an ordinal (integer)\n",
      " |    encoding of the categorical features.\n",
      " |  sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
      " |    dictionary items (also handles string-valued features).\n",
      " |  sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
      " |    encoding of dictionary items or strings.\n",
      " |  sklearn.preprocessing.LabelBinarizer : binarizes labels in a one-vs-all\n",
      " |    fashion.\n",
      " |  sklearn.preprocessing.MultiLabelBinarizer : transforms between iterable of\n",
      " |    iterables and a multilabel format, e.g. a (samples x classes) binary\n",
      " |    matrix indicating the presence of a class label.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OneHotEncoder\n",
      " |      _BaseEncoder\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_values=None, categorical_features=None, categories=None, drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to determine the categories of each feature.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X, then transform X.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X) but more convenient.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : sparse matrix if sparse=True else a 2-d array\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  get_feature_names(self, input_features=None)\n",
      " |      Return feature names for output features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : list of string, length n_features, optional\n",
      " |          String names for input features if available. By default,\n",
      " |          \"x0\", \"x1\", ... \"xn_features\" is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      output_feature_names : array of string, length n_output_features\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the back data to the original representation.\n",
      " |      \n",
      " |      In case unknown categories are encountered (all zeros in the\n",
      " |      one-hot encoding), ``None`` is used to represent this category.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n",
      " |          The transformed data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : array-like, shape [n_samples, n_features]\n",
      " |          Inverse transformed array.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X using one-hot encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : sparse matrix if sparse=True else a 2-d array\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  active_features_\n",
      " |  \n",
      " |  feature_indices_\n",
      " |  \n",
      " |  n_values_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How do we convert this to numerical features?\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "help(OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: [array(['Female', 'Male', 'Unknown'], dtype=object), array(['Chrome', 'Internet Explorer', 'Safari'], dtype=object)]\n",
      "feature names: ['x0_Female' 'x0_Male' 'x0_Unknown' 'x1_Chrome' 'x1_Internet Explorer'\n",
      " 'x1_Safari']\n",
      "[[0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['Firefox'] in column 1 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c14adcdbeb50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# transform X_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX_test_ohe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_ohe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    730\u001b[0m                                        copy=True)\n\u001b[1;32m    731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform_new\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;34m\"\"\"New implementation assuming categorical input\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    120\u001b[0m                     msg = (\"Found unknown categories {0} in column {1}\"\n\u001b[1;32m    121\u001b[0m                            \" during transform\".format(diff, i))\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# Set the problematic rows to an acceptable value and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories ['Firefox'] in column 1 during transform"
     ]
    }
   ],
   "source": [
    "# initialize the encoder\n",
    "enc = OneHotEncoder(sparse=False) # by default, OneHotEncoder returns a sparse matrix. sparse=False returns a 2D array\n",
    "# fit the training data\n",
    "enc.fit(X_train)\n",
    "print('categories:',enc.categories_)\n",
    "print('feature names:',enc.get_feature_names())\n",
    "# transform X_train\n",
    "X_train_ohe = enc.transform(X_train)\n",
    "print(X_train_ohe)\n",
    "\n",
    "# do all of this in one step\n",
    "X_train_ohe = enc.fit_transform(X_train)\n",
    "#print(X_train_ohe)\n",
    "\n",
    "# transform X_test\n",
    "X_test_ohe = enc.transform(X_test)\n",
    "print(X_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "Would you use the OneHotEncorder or the OrdinalEncoder for the following categorical features?\n",
    "- marital status (Married, Divorced, Never-married, Separated, Widowed)\n",
    "- exterior quality of a house (Excellent, Good, Average/Typical, Fair, Poor)\n",
    "- native country (USA, Hungary, China, India, Germany)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this course, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>describe why preprocessing is necessary</font>\n",
    "- <font color='LIGHTGRAY'>apply one-hot encoding or ordinal encoding to categorical variables</font>\n",
    "- **apply scaling and normalization to continuous variables**\n",
    "- <font color='LIGHTGRAY'>apply label encoding to a categorical target variable</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous features: MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume we have two continuous features:\n",
    "train = {'age':[32,65,13,68,42,75,32],'number of hours worked':[0,40,10,60,40,20,40]}\n",
    "test = {'age':[83,26,10,60],'number of hours worked':[0,40,0,60]}\n",
    "\n",
    "X_train = pd.DataFrame(train)\n",
    "X_test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the continuous feature values are reasonably bounded, MinMaxScaler is a good way to scale the features.\n",
    "\n",
    "Age is expected to be within the range of 0 and 100.\n",
    "\n",
    "Number of hours worked per week is in the range of 0 to 80.\n",
    "\n",
    "If unsure, plot the histogram of the feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MinMaxScaler in module sklearn.preprocessing.data:\n",
      "\n",
      "class MinMaxScaler(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  Transforms features by scaling each feature to a given range.\n",
      " |  \n",
      " |  This estimator scales and translates each feature individually such\n",
      " |  that it is in the given range on the training set, e.g. between\n",
      " |  zero and one.\n",
      " |  \n",
      " |  The transformation is given by::\n",
      " |  \n",
      " |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      " |      X_scaled = X_std * (max - min) + min\n",
      " |  \n",
      " |  where min, max = feature_range.\n",
      " |  \n",
      " |  The transformation is calculated as::\n",
      " |  \n",
      " |      X_scaled = scale * X + min - X.min(axis=0) * scale\n",
      " |      where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n",
      " |  \n",
      " |  This transformation is often used as an alternative to zero mean,\n",
      " |  unit variance scaling.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  feature_range : tuple (min, max), default=(0, 1)\n",
      " |      Desired range of transformed data.\n",
      " |  \n",
      " |  copy : boolean, optional, default True\n",
      " |      Set to False to perform inplace row normalization and avoid a\n",
      " |      copy (if the input is already a numpy array).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  min_ : ndarray, shape (n_features,)\n",
      " |      Per feature adjustment for minimum. Equivalent to\n",
      " |      ``min - X.min(axis=0) * self.scale_``\n",
      " |  \n",
      " |  scale_ : ndarray, shape (n_features,)\n",
      " |      Per feature relative scaling of the data. Equivalent to\n",
      " |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  data_min_ : ndarray, shape (n_features,)\n",
      " |      Per feature minimum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_min_*\n",
      " |  \n",
      " |  data_max_ : ndarray, shape (n_features,)\n",
      " |      Per feature maximum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_max_*\n",
      " |  \n",
      " |  data_range_ : ndarray, shape (n_features,)\n",
      " |      Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_range_*\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      " |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      " |  >>> scaler = MinMaxScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      " |  >>> print(scaler.data_max_)\n",
      " |  [ 1. 18.]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[0.   0.  ]\n",
      " |   [0.25 0.25]\n",
      " |   [0.5  0.5 ]\n",
      " |   [1.   1.  ]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[1.5 0. ]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  minmax_scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MinMaxScaler\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, feature_range=(0, 1), copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the minimum and maximum to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to compute the per-feature minimum and maximum\n",
      " |          used for later scaling along the features axis.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Undo the scaling of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          Input data that will be transformed. It cannot be sparse.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of min and max on X for later scaling.\n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when `fit` is not feasible due to very large number of `n_samples`\n",
      " |      or because X is read from a continuous stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y\n",
      " |          Ignored\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scaling features of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          Input data that will be transformed.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "help(MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30645161 0.        ]\n",
      " [0.83870968 0.66666667]\n",
      " [0.         0.16666667]\n",
      " [0.88709677 1.        ]\n",
      " [0.46774194 0.66666667]\n",
      " [1.         0.33333333]\n",
      " [0.30645161 0.66666667]]\n",
      "[[ 1.12903226  0.        ]\n",
      " [ 0.20967742  0.66666667]\n",
      " [-0.0483871   0.        ]\n",
      " [ 0.75806452  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "print(scaler.transform(X_train))\n",
    "print(scaler.transform(X_test)) # note how scaled X_test contains values larger than 1 and smaller than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous features: StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the continuous feature values follow a tailed distribution, StandardScaler is better to use!\n",
    "\n",
    "Salaries are a good example. Most people earn less than 100k but there are a small number of super-rich people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {'salary':[50_000,75_000,40_000,1_000_000,30_000,250_000,35_000,45_000]}\n",
    "test = {'salary':[25_000,55_000,1_500_000,60_000]}\n",
    "\n",
    "X_train = pd.DataFrame(train)\n",
    "X_test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StandardScaler in module sklearn.preprocessing.data:\n",
      "\n",
      "class StandardScaler(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  Standardize features by removing the mean and scaling to unit variance\n",
      " |  \n",
      " |  The standard score of a sample `x` is calculated as:\n",
      " |  \n",
      " |      z = (x - u) / s\n",
      " |  \n",
      " |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      " |  and `s` is the standard deviation of the training samples or one if\n",
      " |  `with_std=False`.\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by computing\n",
      " |  the relevant statistics on the samples in the training set. Mean and\n",
      " |  standard deviation are then stored to be used on later data using the\n",
      " |  `transform` method.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators: they might behave badly if the\n",
      " |  individual features do not more or less look like standard normally\n",
      " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      " |  \n",
      " |  For instance many elements used in the objective function of\n",
      " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      " |  all features are centered around 0 and have variance in the same\n",
      " |  order. If a feature has a variance that is orders of magnitude larger\n",
      " |  that others, it might dominate the objective function and make the\n",
      " |  estimator unable to learn from other features correctly as expected.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : boolean, optional, default True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  with_mean : boolean, True by default\n",
      " |      If True, center the data before scaling.\n",
      " |      This does not work (and will raise an exception) when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_std : boolean, True by default\n",
      " |      If True, scale the data to unit variance (or equivalently,\n",
      " |      unit standard deviation).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray or None, shape (n_features,)\n",
      " |      Per feature relative scaling of the data. This is calculated using\n",
      " |      `np.sqrt(var_)`. Equal to ``None`` when ``with_std=False``.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_*\n",
      " |  \n",
      " |  mean_ : ndarray or None, shape (n_features,)\n",
      " |      The mean value for each feature in the training set.\n",
      " |      Equal to ``None`` when ``with_mean=False``.\n",
      " |  \n",
      " |  var_ : ndarray or None, shape (n_features,)\n",
      " |      The variance for each feature in the training set. Used to compute\n",
      " |      `scale_`. Equal to ``None`` when ``with_std=False``.\n",
      " |  \n",
      " |  n_samples_seen_ : int or array, shape (n_features,)\n",
      " |      The number of samples processed by the estimator for each feature.\n",
      " |      If there are not missing samples, the ``n_samples_seen`` will be an\n",
      " |      integer, otherwise it will be an array.\n",
      " |      Will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      " |  >>> scaler = StandardScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      " |  >>> print(scaler.mean_)\n",
      " |  [0.5 0.5]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[-1. -1.]\n",
      " |   [-1. -1.]\n",
      " |   [ 1.  1.]\n",
      " |   [ 1.  1.]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[3. 3.]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`sklearn.decomposition.PCA`\n",
      " |      Further removes the linear correlation across features with 'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  We use a biased estimator for the standard deviation, equivalent to\n",
      " |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      " |  affect model performance.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, copy=True, with_mean=True, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the mean and std to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y\n",
      " |          Ignored\n",
      " |  \n",
      " |  inverse_transform(self, X, copy=None)\n",
      " |      Scale back the data to the original representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : array-like, shape [n_samples, n_features]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of mean and std on X for later scaling.\n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when `fit` is not feasible due to very large number of `n_samples`\n",
      " |      or because X is read from a continuous stream.\n",
      " |      \n",
      " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      " |      for computing the sample variance: Analysis and recommendations.\"\n",
      " |      The American Statistician 37.3 (1983): 242-247:\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y\n",
      " |          Ignored\n",
      " |  \n",
      " |  transform(self, X, copy=None)\n",
      " |      Perform standardization by centering and scaling\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44873188]\n",
      " [-0.36895732]\n",
      " [-0.4806417 ]\n",
      " [ 2.58270127]\n",
      " [-0.51255153]\n",
      " [ 0.18946457]\n",
      " [-0.49659661]\n",
      " [-0.46468679]]\n",
      "[[-0.52850644]\n",
      " [-0.43277697]\n",
      " [ 4.1781924 ]\n",
      " [-0.41682206]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(X_train))\n",
    "print(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Would you use MinMaxScaler or StandardScaler for the following features?\n",
    "- number of minutes spent on the website\n",
    "- number of days a year spent abroad\n",
    "- USD donated to charity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this course, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>describe why preprocessing is necessary</font>\n",
    "- <font color='LIGHTGRAY'>apply one-hot encoding or ordinal encoding to categorical variables</font>\n",
    "- <font color='LIGHTGRAY'>apply scaling and normalization to continuous variables</font>\n",
    "- **apply label encoding to a categorical target variable**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical target variable: LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification labels need to be integers with value between 0 and n_classes-1.\n",
    "\n",
    "The label is sometimes categorical and sometimes numerical outside the range of 0 and n_classes-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = ['>50K','>50K', '>50K', '<=50K', '<=50K', '>50K']\n",
    "y_test = ['<=50K','>50K','>50K','<=50K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LabelEncoder in module sklearn.preprocessing.label:\n",
      "\n",
      "class LabelEncoder(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  Encode labels with value between 0 and n_classes-1.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_targets>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array of shape (n_class,)\n",
      " |      Holds the label for each class.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  `LabelEncoder` can be used to normalize labels.\n",
      " |  \n",
      " |  >>> from sklearn import preprocessing\n",
      " |  >>> le = preprocessing.LabelEncoder()\n",
      " |  >>> le.fit([1, 2, 2, 6])\n",
      " |  LabelEncoder()\n",
      " |  >>> le.classes_\n",
      " |  array([1, 2, 6])\n",
      " |  >>> le.transform([1, 1, 2, 6]) #doctest: +ELLIPSIS\n",
      " |  array([0, 0, 1, 2]...)\n",
      " |  >>> le.inverse_transform([0, 0, 1, 2])\n",
      " |  array([1, 1, 2, 6])\n",
      " |  \n",
      " |  It can also be used to transform non-numerical labels (as long as they are\n",
      " |  hashable and comparable) to numerical labels.\n",
      " |  \n",
      " |  >>> le = preprocessing.LabelEncoder()\n",
      " |  >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
      " |  LabelEncoder()\n",
      " |  >>> list(le.classes_)\n",
      " |  ['amsterdam', 'paris', 'tokyo']\n",
      " |  >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) #doctest: +ELLIPSIS\n",
      " |  array([2, 2, 1]...)\n",
      " |  >>> list(le.inverse_transform([2, 2, 1]))\n",
      " |  ['tokyo', 'tokyo', 'paris']\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.preprocessing.OrdinalEncoder : encode categorical features\n",
      " |      using a one-hot or ordinal encoding scheme.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LabelEncoder\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, y)\n",
      " |      Fit label encoder\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  fit_transform(self, y)\n",
      " |      Fit label encoder and return encoded labels\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |  \n",
      " |  inverse_transform(self, y)\n",
      " |      Transform labels back to original encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : numpy array of shape [n_samples]\n",
      " |  \n",
      " |  transform(self, y)\n",
      " |      Transform labels to normalized encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "help(LabelEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 1]\n",
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "print(le.fit_transform(y_train))\n",
    "print(le.transform(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "```python\n",
    "y = [2,2,5,8,8,2]\n",
    "le = LabelEncoder()\n",
    "print(le.fit_transform(y))\n",
    "```\n",
    "\n",
    "What will be printed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How and when to do preprocessing in the ML pipeline?\n",
    "\n",
    "- **APPLY TRANSFORMER.FIT ONLY ON YOUR TRAINING DATA!** Then transform the CV and test sets.\n",
    "- One of the most common mistake practitioners make is leaking statistics!\n",
    "     - fit_transform is applied to the whole dataset.\n",
    "     - Then the data is split into train/CV/test.\n",
    "- This is wrong because the properties of the CV and test set (e.g., mean and stdev) should not influence how the training set is transformed.\n",
    "   - You will see later this semester that **leaking statistics produces a model that does not generalize well to previously unseen data.**\n",
    "   - Keep your sets cleanly separated to avoid this mistake!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "#### Now you can\n",
    "- describe why preprocessing is necessary\n",
    "- apply one-hot encoding or ordinal encoding to categorical variables\n",
    "- apply scaling and normalization to continuous variables\n",
    "- apply label encoding to a categorical target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-learn's pipelines\n",
    "\n",
    "The steps in the ML pipleine can be chained together into a scikit-learn pipeline which consists of transformers and one final estimator which is usually your classifier or regression model.\n",
    "\n",
    "It serves multiple purposes but we only discuss one for now:\n",
    "- leaking statistics is safely avoided with pipelines.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Author: Pedro Morales <part.morales@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data from Titanic dataset.\n",
    "titanic_url = ('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "data = pd.read_csv(titanic_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = ['age', 'fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.790\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#prep = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "#print(prep.fit_transform(X_train,y_train))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
